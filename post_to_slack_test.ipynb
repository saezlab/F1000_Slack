{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "args = SimpleNamespace(zotero_library_id=\"5820275\",\n",
    "                       zotero_api_key=\"---\",\n",
    "                       file_path =\"./test_out.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import re\n",
    "from pyzotero import zotero\n",
    "from slack_sdk.webhook import WebhookClient\n",
    "\n",
    "def validate_inputs(file_path, api_key, library_id):\n",
    "    \"\"\"Validate user inputs.\"\"\"\n",
    "    if not file_path:\n",
    "        raise ValueError(\"Input file path is required.\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"Zotero API key is required.\")\n",
    "    if not library_id:\n",
    "        raise ValueError(\"Zotero library ID is required.\")\n",
    "\n",
    "def post_to_slack(webhook_url, publications):\n",
    "    \"\"\"Post publication details to Slack via webhook.\"\"\"\n",
    "    webhook = WebhookClient(webhook_url)\n",
    "    for pub in publications:\n",
    "        details = (\n",
    "            f\"*{pub['data'].get('title', 'No Title')}*\"\n",
    "            f\"Author(s): {', '.join([creator.get('lastName', 'Unknown') for creator in pub['data'].get('creators', [])])}\\n\"\n",
    "            f\"Published in: {pub['data'].get('publicationTitle', 'Unknown')} ({pub['data'].get('date', 'No Date')})\\n\"\n",
    "            f\"URL: {pub['data'].get('url', 'No URL')}\\n\"\n",
    "        )\n",
    "        response = webhook.send(text=details)\n",
    "        if response.status_code != 200 or response.body != \"ok\":\n",
    "            print(f\"Failed to post to Slack: {response.body}\")\n",
    "            break\n",
    "\n",
    "def fetch_new_publications(zot, last_date):\n",
    "    \"\"\"Fetch new publications from Zotero since the last date.\"\"\"\n",
    "    items = zot.top(limit=100, sort='dateAdded', direction='desc')\n",
    "    new_items = []\n",
    "    try:\n",
    "        # Convert last_date (milliseconds) to offset-aware datetime\n",
    "        last_date_dt = datetime.fromtimestamp(int(last_date) / 1000).replace(tzinfo=None)\n",
    "        for item in items:\n",
    "            # Convert Zotero's dateAdded to offset-naive datetime\n",
    "            item_date = datetime.fromisoformat(item['data']['dateAdded'].replace(\"Z\", \"+00:00\")).replace(tzinfo=None)\n",
    "            if item_date > last_date_dt:\n",
    "                new_items.append(item)\n",
    "    except Exception as e:\n",
    "        print(f\"Date parsing error: {e}\")\n",
    "    return new_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    # Parse command-line arguments\n",
    "    parser = argparse.ArgumentParser(description=\"Post new Zotero publications to Slack.\")\n",
    "    parser.add_argument(\"--file_path\", required=True, help=\"Path to the input CSV file containing webhooks and last date.\")\n",
    "    parser.add_argument(\"--zotero_api_key\", required=True, help=\"Zotero API key.\")\n",
    "    parser.add_argument(\"--zotero_library_id\", required=True, help=\"Zotero library ID.\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Validate inputs\n",
    "    try:\n",
    "        validate_inputs(args.file_path, args.zotero_api_key, args.zotero_library_id)\n",
    "    except Exception as e:\n",
    "        print(f\"Input validation failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # Initialize Zotero API\n",
    "    zot = zotero.Zotero(args.zotero_library_id, 'group', args.zotero_api_key)\n",
    "\n",
    "    # Read input file\n",
    "    try:\n",
    "        webhooks_df = pd.read_csv(args.file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read input file: {e}\")\n",
    "        return\n",
    "\n",
    "    updated_dates = []\n",
    "\n",
    "    for _, row in webhooks_df.iterrows():\n",
    "        last_date = row[\"lastDate\"]\n",
    "        webhook_url = row[\"webhook\"]\n",
    "\n",
    "        # Fetch new publications\n",
    "        try:\n",
    "            new_pubs = fetch_new_publications(zot, last_date)\n",
    "            print(f\"Found {len(new_pubs)} new publications for webhook {webhook_url}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch publications: {e}\")\n",
    "            updated_dates.append(last_date)  # Append current date if fetching fails\n",
    "            continue\n",
    "\n",
    "        # Post to Slack\n",
    "        try:\n",
    "            post_to_slack(webhook_url, new_pubs)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to post to Slack: {e}\")\n",
    "            updated_dates.append(last_date)  # Append current date if posting fails\n",
    "            continue\n",
    "\n",
    "        # Update last date\n",
    "        if new_pubs:\n",
    "            latest_date = int(datetime.fromisoformat(new_pubs[-1]['data']['dateAdded'].replace(\"Z\", \"+00:00\")).timestamp() * 1000)\n",
    "            updated_dates.append(latest_date)\n",
    "        else:\n",
    "            updated_dates.append(last_date)\n",
    "\n",
    "    # Ensure updated_dates matches the DataFrame length\n",
    "    if len(updated_dates) != len(webhooks_df):\n",
    "        print(\"Error: Mismatch between updated_dates and DataFrame length.\")\n",
    "        return\n",
    "\n",
    "    # Update the DataFrame and save back to the CSV file\n",
    "    webhooks_df[\"lastDate\"] = updated_dates\n",
    "    try:\n",
    "        webhooks_df.to_csv(args.file_path, index=False)\n",
    "        print(f\"Updated file saved to {args.file_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save updated file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "zot = zotero.Zotero(args.zotero_library_id, 'group', args.zotero_api_key)\n",
    "webhooks_df = pd.read_csv(args.file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in webhooks_df.iterrows():\n",
    "    last_date = row[\"lastDate\"]\n",
    "    webhook_url = row[\"webhook\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_new_publications(zot, last_date):\n",
    "    \"\"\"Fetch new publications from Zotero since the last date.\"\"\"\n",
    "    items = zot.top(limit=100, sort='dateAdded', direction='desc')\n",
    "    new_items = []\n",
    "    try:\n",
    "        # Convert last_date (milliseconds) to offset-aware datetime\n",
    "        last_date_dt = datetime.fromtimestamp(int(last_date) / 1000).replace(tzinfo=None)\n",
    "        for item in items:\n",
    "            # Convert Zotero's dateAdded to offset-naive datetime\n",
    "            item_date = datetime.fromisoformat(item['data']['dateAdded'].replace(\"Z\", \"+00:00\")).replace(tzinfo=None)\n",
    "            if item_date > last_date_dt:\n",
    "                new_items.append(item)\n",
    "    except Exception as e:\n",
    "        print(f\"Date parsing error: {e}\")\n",
    "    return new_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date = \"1000000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pubs = fetch_new_publications(zot, last_date)\n",
    "print(f\"Found {len(new_pubs)} new publications for webhook {webhook_url}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pubs = fetch_new_publications(zot, last_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(new_pubs)):\n",
    "    ja = new_pubs[i]['data'].get('journalAbbreviation', 'Unknown')\n",
    "    print(ja)\n",
    "    ja = new_pubs[i]['data'].get('publicationTitle', 'Unknown')\n",
    "    print(ja)\n",
    "    a = new_pubs[i]['data'].get('publisher', 'Unknown')\n",
    "    print(ja)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pubs[2].get('links').get('alternate').get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "webhook = WebhookClient(webhook_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from slack_sdk import WebClient\n",
    "from slack_sdk.errors import SlackApiError\n",
    "slack_token = \"something starting with xoxb\"\n",
    "client = WebClient(token=slack_token)\n",
    "\n",
    "response  = client.users_list()\n",
    "#result = client.users_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "members2 = []\n",
    "for member in response.get('members'):\n",
    "    if not member.get('deleted'):\n",
    "        members2.append(member)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.get('members')[1].get(\"id\"))\n",
    "\n",
    "\n",
    "# Collect real_name_normalized for all members that are not deleted\n",
    "members = []\n",
    "for member in response.get('members'):\n",
    "    if not member.get('deleted'):\n",
    "        members.append(member.get(\"profile\").get(\"display_name_normalized\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_names_in_notes(notes, slack_users_df):\n",
    "    \"\"\"Replace names in notes with matches from Slack users, inserting user IDs.\"\"\"\n",
    "    def find_best_match(name):\n",
    "        name_cleaned = name.lstrip(\"@\").lower()\n",
    "        best_match = None\n",
    "        highest_score = 0\n",
    "        for _, row in slack_users_df.iterrows():\n",
    "            normalized_cleaned = row[\"display_name_normalized\"].replace(\" \", \"\").lower()\n",
    "            score = fuzz.ratio(name_cleaned, normalized_cleaned)\n",
    "            if score > highest_score and score >= 50:  # Threshold of 50\n",
    "                highest_score = score\n",
    "                best_match = row[\"id\"]\n",
    "        return best_match\n",
    "\n",
    "    def replacer(match):\n",
    "        name = match.group(0)\n",
    "        matched_id = find_best_match(name)\n",
    "        if matched_id:\n",
    "            return f\"<@{matched_id}>\"\n",
    "        return name\n",
    "\n",
    "    return re.sub(r\"@\\w+\", replacer, notes)\n",
    "\n",
    "def get_slack_users(slack_token):\n",
    "    \"\"\"Fetch Slack users and return a DataFrame with display_name_normalized and id.\"\"\"\n",
    "    client = WebClient(token=slack_token)\n",
    "    response = client.users_list()\n",
    "\n",
    "    members = []\n",
    "    for member in response.get('members', []):\n",
    "        if not member.get('deleted'):\n",
    "            profile = member.get(\"profile\", {})\n",
    "            display_name = profile.get(\"display_name_normalized\")\n",
    "            user_id = member.get(\"id\")\n",
    "            if display_name and user_id:\n",
    "                members.append({\"display_name_normalized\": display_name, \"id\": user_id})\n",
    "\n",
    "    # Convert the list of members to a DataFrame\n",
    "    members_df = pd.DataFrame(members)\n",
    "    return members_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "slack_users = get_slack_users(slack_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slack_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_names_in_notes(\"esting to share a paper via zotero. Picked one that I am not sure about how meaningful it is - Deep learning enrichmentâ€¦ @Attila what do you think ;-)\",slack_users)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_str = replace_names_in_notes(notes_str, slack_users)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webhook.send(\n",
    "            text=\"fallback\",\n",
    "            blocks=[\n",
    "                {\n",
    "                    \"type\": \"section\",\n",
    "                    \"text\": {\n",
    "                        \"type\": \"mrkdwn\",\n",
    "                        \"text\": \"<@Attila>\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zotero_access",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
